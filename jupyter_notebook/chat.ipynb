{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unstructured==0.7.12\n",
    "%pip install langchain\n",
    "%pip install openai\n",
    "%pip install chromadb\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory, VectorStoreRetrieverMemory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat Module is the conversational pipeline that tracks chosen steps in proof and selects the next step to be proved. It will use a router chain to add the choice to the context module to provide more recent context in the process. \n",
    "\n",
    "It has weaker memory of the pdf, but can refer to context module for other references\n",
    "\n",
    "Might not need chat module since we can just sent proof decision to context module directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../doc/', glob='linear_map.pdf')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "# Create embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "# load it into Chroma\n",
    "vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = openai_api_key), chain_type=\"stuff\", vectorstore=vectorstore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "# For conversation asking at a specific step in proof\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True, retriever=retriever)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a chatbot having a conversation with a human.\"), # The persistent system prompt\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # Where the memory will be stored.\n",
    "    HumanMessagePromptTemplate.from_template(\"{human_input}\"), # Where the human input will injectd\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\", openai_api_key=openai_api_key)\n",
    "# get context of the past proof steps\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is my name\"\n",
    "llm_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
